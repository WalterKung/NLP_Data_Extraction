{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.learner import *\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "import dill as pickle\n",
    "import spacy\n",
    "\n",
    "\n",
    "PATH='/home/wk/myProjects/data/Enron/oper/'\n",
    "\n",
    "TRN_PATH = 'train/'\n",
    "VAL_PATH = 'test/'\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'\n",
    "\n",
    "bs=32; bptt=75\n",
    "em_sz = 300  # size of each embedding vector\n",
    "nh = 500     # number of hidden activations per layer\n",
    "nl = 3       # number of layers\n",
    "\n",
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "TEXT = pickle.load(open(f'{PATH}models/TEXT.pkl','rb'))\n",
    "\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=10)\n",
    "\n",
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "learner = md.get_model(opt_fn, em_sz, nh, nl,\n",
    "               dropouti=0.05, dropout=0.05, wdrop=0.1, dropoute=0.02, dropouth=0.05)\n",
    "learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learner.clip=0.3\n",
    "\n",
    "learner.load_encoder('adam3_10_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH=Path('/home/wk/myProjects/data/Enron/tag/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[str(i) for i in PATH.iterdir()]\n",
    "#PATH/'here.txt'.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'message-id: <32399675.1075856463241.javamail.evans@thyme> date: fri, 22 dec 2000 09:58:00 -0800 (pst) from: vince.kaminski@enron.com to: tanya.tamarchenko@enron.com subject: re: suggestion: implementing var based on non-normal log-returns simulations mime-version: 1.0 content-type: text/plain; charset=us-ascii content-transfer-encoding: 7bit x-from: vince j kaminski x-to: tanya tamarchenko x-cc: x-bcc: x-folder: \\\\vincent_kaminski_jun2001_3\\\\notes folders\\\\sent x-origin: kaminski-v x-filename: vkamins.nsf tanya, i could not locate the file. vince tanya tamarchenko 12/07/2000 01:17 pm to: vince j kaminski/hou/ect@ect, rabi de/na/enron@enron, jaesoo lew/na/enron@enron cc: subject: re: suggestion: implementing var based on non-normal log-returns simulations everybody, we were talking for a while about using non-normal distributions in the monte-carlo simulations in our var model. i put together some suggestion regarding this. the text is under o:\\\\_dropbox\\\\tanya\\\\non_normal_logs.doc look through this 3 page document, and let me know what you think, please. tanya'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test\n",
    "ss=r\"\"\"\n",
    "Message-ID: <32399675.1075856463241.JavaMail.evans@thyme>\n",
    "Date: Fri, 22 Dec 2000 09:58:00 -0800 (PST)\n",
    "From: vince.kaminski@enron.com\n",
    "To: tanya.tamarchenko@enron.com\n",
    "Subject: Re: Suggestion: implementing VAR based on non-normal log-returns\n",
    " simulations\n",
    "Mime-Version: 1.0\n",
    "Content-Type: text/plain; charset=us-ascii\n",
    "Content-Transfer-Encoding: 7bit\n",
    "X-From: Vince J Kaminski\n",
    "X-To: Tanya Tamarchenko\n",
    "X-cc: \n",
    "X-bcc: \n",
    "X-Folder: \\Vincent_Kaminski_Jun2001_3\\Notes Folders\\Sent\n",
    "X-Origin: Kaminski-V\n",
    "X-FileName: vkamins.nsf\n",
    "\n",
    "Tanya,\n",
    "\n",
    "I could not locate the file.\n",
    "\n",
    "Vince\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tanya Tamarchenko\n",
    "12/07/2000 01:17 PM\n",
    "To: Vince J Kaminski/HOU/ECT@ECT, Rabi De/NA/Enron@ENRON, Jaesoo \n",
    "Lew/NA/Enron@ENRON\n",
    "cc:  \n",
    "Subject: Re: Suggestion: implementing VAR based on non-normal log-returns \n",
    "simulations  \n",
    "\n",
    "Everybody, \n",
    "we were talking for a while about using non-normal distributions in the \n",
    "Monte-Carlo simulations in our VAR model.\n",
    "I put together some suggestion regarding this. The text is under \n",
    "O:\\_Dropbox\\Tanya\\non_normal_logs.doc\n",
    "\n",
    "Look through this 3 page document, and let me know what you think, please.\n",
    "\n",
    "\n",
    "Tanya\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "s = [TEXT.preprocess(ss)] \n",
    "t=TEXT.numericalize(s)\n",
    "' '.join(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos=4; @@othr_dt@@ <- 22 \n",
      "pos=5; @@othr_tm@@ <- dec \n",
      "pos=11; @@email@@ <- vince.kaminski@enron.com \n",
      "pos=13; @@othr_em@@ <- tanya.tamarchenko@enron.com \n",
      "pos=32; @@firstname@@ <- vince \n",
      "pos=33; @@lastname@@ <- j \n",
      "pos=57; @@othr_dt@@ <- 12/07/2000 \n",
      "pos=58; @@othr_tm@@ <- 01:17 \n",
      "pos=61; @@firstname@@ <- vince \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "ss = copy.deepcopy(s)\n",
    "s_p = [ss[0][0:3]]\n",
    "t=TEXT.numericalize(s)\n",
    "#t = t.long().cpu()\n",
    "\n",
    "m=learner.model\n",
    "\n",
    "m[0].bs=1\n",
    "m.eval()\n",
    "m.reset()\n",
    "\n",
    "#m.cpu()\n",
    "#m.cuda()\n",
    "res,*_ = m(t)\n",
    "\n",
    "nexts = torch.topk(res[-1], 10)[1]\n",
    "\n",
    "for i in range(len(s[0])-3):\n",
    "    n=res[-1].topk(2)[1]    \n",
    "    predict = TEXT.vocab.itos[n.data[0]]    \n",
    "    if (predict[0:2] == '@@'):\n",
    "        print(\"pos=\" + str(i+3) + \"; \" + predict + \" <- \" + s[0][i+3] , end=' \\n')                    \n",
    "        ss[0][i+3]=predict\n",
    "    \n",
    "    s_p = [ss[0][0:i+4]]\n",
    "    t=TEXT.numericalize(s_p)\n",
    "\n",
    "    res,*_ = m(t)    \n",
    "    \n",
    "m[0].bs=bs       \n",
    "print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://askubuntu.com/questions/607118/cuda-not-working-after-returning-laptop-from-sleep#750939\n",
    "len(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sudo rmmod nvidia_uvm\n",
    "#sudo modprobe nvidia_uvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t.long().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
